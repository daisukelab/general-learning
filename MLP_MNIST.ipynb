{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchMLPClassifier example - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlcliche.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP.torch_mlp_clf import TorchMLPClassifier\n",
    "import torchvision\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-split of train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0001/200: lr: 0.0010000: loss=71.942265 val_acc=0.9403333 val_loss=38.7114067\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0002/200: lr: 0.0010000: loss=27.704687 val_acc=0.9540000 val_loss=29.5278530\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0003/200: lr: 0.0010000: loss=19.087200 val_acc=0.9601667 val_loss=26.8306103\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0004/200: lr: 0.0010000: loss=14.016612 val_acc=0.9648333 val_loss=24.7851257\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0005/200: lr: 0.0010000: loss=10.624942 val_acc=0.9641667 val_loss=25.2494850\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0006/200: lr: 0.0010000: loss=8.334893 val_acc=0.9648333 val_loss=25.4945278\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0007/200: lr: 0.0010000: loss=6.518493 val_acc=0.9680000 val_loss=25.4847927\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0008/200: lr: 0.0010000: loss=5.243613 val_acc=0.9680000 val_loss=26.1397228\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0009/200: lr: 0.0010000: loss=4.241420 val_acc=0.9708333 val_loss=27.1096897\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0010/200: lr: 0.0010000: loss=3.248130 val_acc=0.9688333 val_loss=27.4381142\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0011/200: lr: 0.0010000: loss=2.626857 val_acc=0.9693333 val_loss=28.3022652\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0012/200: lr: 0.0010000: loss=2.055179 val_acc=0.9708333 val_loss=28.8885574\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0013/200: lr: 0.0010000: loss=1.682043 val_acc=0.9718333 val_loss=30.2040596\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0014/200: lr: 0.0010000: loss=1.390999 val_acc=0.9693333 val_loss=31.2438087\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0015/200: lr: 0.0010000: loss=1.122315 val_acc=0.9695000 val_loss=30.9188519\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0016/200: lr: 0.0010000: loss=0.860031 val_acc=0.9703333 val_loss=31.7269669\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0017/200: lr: 0.0010000: loss=0.710484 val_acc=0.9708333 val_loss=31.7040768\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0018/200: lr: 0.0010000: loss=0.594635 val_acc=0.9708333 val_loss=32.2037659\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0019/200: lr: 0.0010000: loss=0.481121 val_acc=0.9713333 val_loss=32.8668213\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0020/200: lr: 0.0010000: loss=0.420276 val_acc=0.9706667 val_loss=33.9991341\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0021/200: lr: 0.0010000: loss=0.683124 val_acc=0.9673333 val_loss=38.5327950\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0022/200: lr: 0.0010000: loss=3.435311 val_acc=0.9641667 val_loss=47.5396957\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0023/200: lr: 0.0010000: loss=3.898932 val_acc=0.9676667 val_loss=43.9909897\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0024/200: lr: 0.0010000: loss=1.749005 val_acc=0.9676667 val_loss=40.5815926\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0025/200: lr: 0.0010000: loss=0.499743 val_acc=0.9705000 val_loss=41.2344589\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0026/200: lr: 0.0010000: loss=0.397056 val_acc=0.9703333 val_loss=41.1618500\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0027/200: lr: 0.0010000: loss=0.198915 val_acc=0.9701667 val_loss=43.1958199\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0028/200: lr: 0.0010000: loss=0.265565 val_acc=0.9706667 val_loss=42.0656929\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0029/200: lr: 0.0010000: loss=0.169990 val_acc=0.9711667 val_loss=42.2118835\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0030/200: lr: 0.0010000: loss=0.118425 val_acc=0.9716667 val_loss=42.7631302\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0031/200: lr: 0.0010000: loss=0.097342 val_acc=0.9726667 val_loss=42.3960419\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0032/200: lr: 0.0010000: loss=0.077250 val_acc=0.9718333 val_loss=43.0077171\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0033/200: lr: 0.0010000: loss=0.067185 val_acc=0.9716667 val_loss=43.0282326\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0034/200: lr: 0.0010000: loss=0.061497 val_acc=0.9725000 val_loss=43.5951462\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0035/200: lr: 0.0010000: loss=0.057554 val_acc=0.9716667 val_loss=43.3669205\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0036/200: lr: 0.0010000: loss=0.050619 val_acc=0.9718333 val_loss=43.8707390\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0037/200: lr: 0.0010000: loss=0.047463 val_acc=0.9723333 val_loss=44.1225815\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0038/200: lr: 0.0010000: loss=0.042878 val_acc=0.9720000 val_loss=44.2815971\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0039/200: lr: 0.0010000: loss=0.038127 val_acc=0.9715000 val_loss=44.9380150\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0040/200: lr: 0.0010000: loss=0.491582 val_acc=0.9663333 val_loss=52.0822411\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0041/200: lr: 0.0010000: loss=6.344560 val_acc=0.9666667 val_loss=59.2082710\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0042/200: lr: 0.0010000: loss=2.617255 val_acc=0.9653333 val_loss=54.2732773\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0043/200: lr: 0.0010000: loss=0.555037 val_acc=0.9686667 val_loss=53.7860107\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0044/200: lr: 0.0010000: loss=0.235592 val_acc=0.9690000 val_loss=50.9896126\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0045/200: lr: 0.0010000: loss=0.123473 val_acc=0.9696667 val_loss=50.9557457\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0046/200: lr: 0.0010000: loss=0.063033 val_acc=0.9703333 val_loss=50.0757866\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0047/200: lr: 0.0010000: loss=0.041966 val_acc=0.9701667 val_loss=49.9894562\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0048/200: lr: 0.0010000: loss=0.036020 val_acc=0.9703333 val_loss=50.0804443\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0049/200: lr: 0.0010000: loss=0.032657 val_acc=0.9703333 val_loss=50.0363312\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0050/200: lr: 0.0010000: loss=0.029733 val_acc=0.9705000 val_loss=49.9745140\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0051/200: lr: 0.0010000: loss=0.027307 val_acc=0.9703333 val_loss=50.0484428\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0052/200: lr: 0.0010000: loss=0.025010 val_acc=0.9705000 val_loss=50.1520996\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0053/200: lr: 0.0010000: loss=0.023228 val_acc=0.9703333 val_loss=50.0796242\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0054/200: lr: 0.0010000: loss=0.021345 val_acc=0.9703333 val_loss=50.1662254\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0055/200: lr: 0.0010000: loss=0.019970 val_acc=0.9705000 val_loss=50.2242889\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0056/200: lr: 0.0010000: loss=0.018299 val_acc=0.9703333 val_loss=50.3185463\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0057/200: lr: 0.0010000: loss=0.017326 val_acc=0.9701667 val_loss=50.6268921\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0058/200: lr: 0.0010000: loss=0.016293 val_acc=0.9703333 val_loss=50.5272522\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0059/200: lr: 0.0010000: loss=0.014567 val_acc=0.9706667 val_loss=50.5159988\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0060/200: lr: 0.0010000: loss=0.013510 val_acc=0.9706667 val_loss=50.4991951\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0061/200: lr: 0.0010000: loss=0.012443 val_acc=0.9705000 val_loss=50.8826675\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0062/200: lr: 0.0010000: loss=0.011309 val_acc=0.9710000 val_loss=50.8380432\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0063/200: lr: 0.0010000: loss=0.010595 val_acc=0.9708333 val_loss=50.7685356\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0064/200: lr: 0.0010000: loss=0.009719 val_acc=0.9710000 val_loss=51.0086708\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0065/200: lr: 0.0010000: loss=0.008875 val_acc=0.9708333 val_loss=51.0514107\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0066/200: lr: 0.0010000: loss=0.008330 val_acc=0.9713333 val_loss=51.1627541\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0067/200: lr: 0.0010000: loss=0.007428 val_acc=0.9710000 val_loss=51.2244415\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0068/200: lr: 0.0010000: loss=0.006893 val_acc=0.9706667 val_loss=51.3874054\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0069/200: lr: 0.0010000: loss=0.006277 val_acc=0.9706667 val_loss=51.3323975\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0070/200: lr: 0.0010000: loss=0.005559 val_acc=0.9706667 val_loss=51.5076218\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0071/200: lr: 0.0010000: loss=8.630782 val_acc=0.9633333 val_loss=71.7161560\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0072/200: lr: 0.0010000: loss=5.246451 val_acc=0.9678333 val_loss=64.1855164\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0073/200: lr: 0.0010000: loss=1.369666 val_acc=0.9688333 val_loss=63.4788170\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0074/200: lr: 0.0010000: loss=0.235588 val_acc=0.9701667 val_loss=62.9690170\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0075/200: lr: 0.0010000: loss=0.085042 val_acc=0.9705000 val_loss=60.8756790\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0076/200: lr: 0.0010000: loss=0.026049 val_acc=0.9705000 val_loss=60.7829781\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0077/200: lr: 0.0010000: loss=0.021154 val_acc=0.9706667 val_loss=60.6270905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0078/200: lr: 0.0010000: loss=0.018544 val_acc=0.9706667 val_loss=60.5627136\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0079/200: lr: 0.0010000: loss=0.016585 val_acc=0.9706667 val_loss=60.4947777\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0080/200: lr: 0.0010000: loss=0.015014 val_acc=0.9708333 val_loss=60.3518143\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0081/200: lr: 0.0010000: loss=0.013715 val_acc=0.9713333 val_loss=60.2960014\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0082/200: lr: 0.0010000: loss=0.012581 val_acc=0.9715000 val_loss=60.2006531\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0083/200: lr: 0.0010000: loss=0.011553 val_acc=0.9711667 val_loss=60.1927414\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0084/200: lr: 0.0010000: loss=0.010705 val_acc=0.9713333 val_loss=60.1564827\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0085/200: lr: 0.0010000: loss=0.009886 val_acc=0.9710000 val_loss=59.9575768\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0086/200: lr: 0.0010000: loss=0.009111 val_acc=0.9710000 val_loss=60.0972672\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0087/200: lr: 0.0010000: loss=0.008461 val_acc=0.9713333 val_loss=59.8695107\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0088/200: lr: 0.0010000: loss=0.007814 val_acc=0.9713333 val_loss=59.7791939\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0089/200: lr: 0.0010000: loss=0.007260 val_acc=0.9715000 val_loss=59.8133507\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0090/200: lr: 0.0010000: loss=0.006754 val_acc=0.9710000 val_loss=59.8446770\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0091/200: lr: 0.0010000: loss=0.006259 val_acc=0.9710000 val_loss=59.6990013\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0092/200: lr: 0.0010000: loss=0.005763 val_acc=0.9713333 val_loss=59.5632439\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0093/200: lr: 0.0010000: loss=0.005314 val_acc=0.9710000 val_loss=59.2871056\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0094/200: lr: 0.0010000: loss=0.004950 val_acc=0.9713333 val_loss=59.2995872\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0095/200: lr: 0.0010000: loss=0.004574 val_acc=0.9715000 val_loss=59.2315025\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0096/200: lr: 0.0010000: loss=0.004191 val_acc=0.9716667 val_loss=59.1892776\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0097/200: lr: 0.0010000: loss=0.003890 val_acc=0.9720000 val_loss=59.1081276\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0098/200: lr: 0.0010000: loss=0.003564 val_acc=0.9720000 val_loss=58.8278160\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0099/200: lr: 0.0010000: loss=0.003288 val_acc=0.9721667 val_loss=58.8488922\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0100/200: lr: 0.0010000: loss=0.003031 val_acc=0.9716667 val_loss=58.8569412\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0101/200: lr: 0.0010000: loss=0.002763 val_acc=0.9721667 val_loss=58.3924789\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0102/200: lr: 0.0010000: loss=0.002586 val_acc=0.9725000 val_loss=58.4433060\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0103/200: lr: 0.0010000: loss=0.002330 val_acc=0.9725000 val_loss=58.2194099\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0104/200: lr: 0.0010000: loss=0.002141 val_acc=0.9725000 val_loss=58.0907784\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0105/200: lr: 0.0010000: loss=0.001975 val_acc=0.9721667 val_loss=58.1909866\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0106/200: lr: 0.0010000: loss=0.001809 val_acc=0.9721667 val_loss=57.7772789\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0107/200: lr: 0.0010000: loss=0.001657 val_acc=0.9721667 val_loss=57.5724411\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0108/200: lr: 0.0010000: loss=0.001475 val_acc=0.9725000 val_loss=57.1945267\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0109/200: lr: 0.0010000: loss=0.001383 val_acc=0.9721667 val_loss=57.3783951\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0110/200: lr: 0.0010000: loss=0.001241 val_acc=0.9723333 val_loss=57.0853386\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0111/200: lr: 0.0010000: loss=0.001127 val_acc=0.9723333 val_loss=57.0660133\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0112/200: lr: 0.0010000: loss=0.001008 val_acc=0.9723333 val_loss=56.8150978\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0113/200: lr: 0.0010000: loss=0.000928 val_acc=0.9725000 val_loss=56.9167213\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0114/200: lr: 0.0010000: loss=0.000852 val_acc=0.9721667 val_loss=56.8824081\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0115/200: lr: 0.0010000: loss=7.007588 val_acc=0.9646667 val_loss=81.2149811\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0116/200: lr: 0.0010000: loss=3.879805 val_acc=0.9670000 val_loss=73.7211990\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0117/200: lr: 0.0010000: loss=1.768138 val_acc=0.9685000 val_loss=73.7037888\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0118/200: lr: 0.0010000: loss=0.260524 val_acc=0.9685000 val_loss=74.0252609\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0119/200: lr: 0.0010000: loss=0.061058 val_acc=0.9695000 val_loss=73.3403015\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0120/200: lr: 0.0010000: loss=0.014302 val_acc=0.9696667 val_loss=73.2336349\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0121/200: lr: 0.0010000: loss=0.010503 val_acc=0.9696667 val_loss=73.0374603\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0122/200: lr: 0.0010000: loss=0.008817 val_acc=0.9693333 val_loss=72.9505920\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0123/200: lr: 0.0010000: loss=0.007713 val_acc=0.9696667 val_loss=72.8255997\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0124/200: lr: 0.0010000: loss=0.006896 val_acc=0.9695000 val_loss=72.7244644\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0125/200: lr: 0.0010000: loss=0.006230 val_acc=0.9696667 val_loss=72.6239395\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0126/200: lr: 0.0010000: loss=0.005692 val_acc=0.9695000 val_loss=72.4991608\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0127/200: lr: 0.0010000: loss=0.005185 val_acc=0.9698333 val_loss=72.4134064\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0128/200: lr: 0.0010000: loss=0.004771 val_acc=0.9700000 val_loss=72.2989655\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0129/200: lr: 0.0010000: loss=0.004389 val_acc=0.9700000 val_loss=72.2221222\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0130/200: lr: 0.0010000: loss=0.004040 val_acc=0.9700000 val_loss=72.0877380\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0131/200: lr: 0.0010000: loss=0.003741 val_acc=0.9700000 val_loss=72.0013733\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0132/200: lr: 0.0010000: loss=0.003455 val_acc=0.9701667 val_loss=71.9040909\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0133/200: lr: 0.0010000: loss=0.003201 val_acc=0.9703333 val_loss=71.8054504\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0134/200: lr: 0.0010000: loss=0.002960 val_acc=0.9706667 val_loss=71.6375275\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0135/200: lr: 0.0010000: loss=0.002749 val_acc=0.9706667 val_loss=71.4904480\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0136/200: lr: 0.0010000: loss=0.002530 val_acc=0.9703333 val_loss=71.3508606\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0137/200: lr: 0.0010000: loss=0.002354 val_acc=0.9703333 val_loss=71.1936951\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0138/200: lr: 0.0010000: loss=0.002174 val_acc=0.9708333 val_loss=71.0298386\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0139/200: lr: 0.0010000: loss=0.002012 val_acc=0.9703333 val_loss=70.9244919\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0140/200: lr: 0.0010000: loss=0.001863 val_acc=0.9705000 val_loss=70.6916809\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0141/200: lr: 0.0010000: loss=0.001724 val_acc=0.9701667 val_loss=70.5224838\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0142/200: lr: 0.0010000: loss=0.001590 val_acc=0.9701667 val_loss=70.2534637\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0143/200: lr: 0.0010000: loss=0.001465 val_acc=0.9703333 val_loss=69.8888321\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0144/200: lr: 0.0010000: loss=0.001358 val_acc=0.9703333 val_loss=69.5548096\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0145/200: lr: 0.0010000: loss=0.001251 val_acc=0.9701667 val_loss=69.3416595\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0146/200: lr: 0.0010000: loss=0.001156 val_acc=0.9703333 val_loss=68.9327316\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0147/200: lr: 0.0010000: loss=0.001064 val_acc=0.9705000 val_loss=68.5159378\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0148/200: lr: 0.0010000: loss=0.000980 val_acc=0.9708333 val_loss=67.8770370\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0149/200: lr: 0.0010000: loss=0.000899 val_acc=0.9701667 val_loss=67.6115036\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0150/200: lr: 0.0010000: loss=0.000831 val_acc=0.9701667 val_loss=67.0230637\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0151/200: lr: 0.0010000: loss=0.000767 val_acc=0.9706667 val_loss=66.5850525\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0152/200: lr: 0.0010000: loss=0.000706 val_acc=0.9705000 val_loss=66.0197220\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0153/200: lr: 0.0010000: loss=0.000647 val_acc=0.9710000 val_loss=65.7363434\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0154/200: lr: 0.0010000: loss=0.000605 val_acc=0.9710000 val_loss=65.3277130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0155/200: lr: 0.0010000: loss=0.000551 val_acc=0.9708333 val_loss=64.9855652\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0156/200: lr: 0.0010000: loss=0.000504 val_acc=0.9710000 val_loss=64.5451126\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0157/200: lr: 0.0010000: loss=0.000467 val_acc=0.9711667 val_loss=63.9825783\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0158/200: lr: 0.0010000: loss=0.000427 val_acc=0.9710000 val_loss=63.7262650\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0159/200: lr: 0.0010000: loss=0.000391 val_acc=0.9713333 val_loss=63.2102280\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0160/200: lr: 0.0010000: loss=0.000358 val_acc=0.9713333 val_loss=62.6813011\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0161/200: lr: 0.0010000: loss=0.000329 val_acc=0.9716667 val_loss=62.4047623\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0162/200: lr: 0.0010000: loss=0.000299 val_acc=0.9718333 val_loss=62.1552353\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0163/200: lr: 0.0010000: loss=0.000276 val_acc=0.9718333 val_loss=61.9410286\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0164/200: lr: 0.0010000: loss=0.000255 val_acc=0.9720000 val_loss=61.4916153\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0165/200: lr: 0.0010000: loss=0.000241 val_acc=0.9720000 val_loss=60.8564873\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0166/200: lr: 0.0010000: loss=2.341607 val_acc=0.9596667 val_loss=94.2988815\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0167/200: lr: 0.0010000: loss=8.071633 val_acc=0.9670000 val_loss=71.5060272\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0168/200: lr: 0.0010000: loss=3.097379 val_acc=0.9688333 val_loss=72.6024170\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0169/200: lr: 0.0010000: loss=1.831103 val_acc=0.9698333 val_loss=71.3640366\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0170/200: lr: 0.0010000: loss=1.397003 val_acc=0.9690000 val_loss=71.9110260\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0171/200: lr: 0.0010000: loss=0.442758 val_acc=0.9691667 val_loss=68.9192429\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0172/200: lr: 0.0010000: loss=0.669583 val_acc=0.9688333 val_loss=73.6348038\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0173/200: lr: 0.0010000: loss=0.115652 val_acc=0.9716667 val_loss=68.7790375\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0174/200: lr: 0.0010000: loss=0.046563 val_acc=0.9711667 val_loss=68.5269928\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0175/200: lr: 0.0010000: loss=0.007583 val_acc=0.9715000 val_loss=68.7000961\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0176/200: lr: 0.0010000: loss=0.005017 val_acc=0.9715000 val_loss=68.6704102\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0177/200: lr: 0.0010000: loss=0.004315 val_acc=0.9713333 val_loss=68.6172485\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0178/200: lr: 0.0010000: loss=0.003811 val_acc=0.9711667 val_loss=68.5852509\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0179/200: lr: 0.0010000: loss=0.003417 val_acc=0.9711667 val_loss=68.5393066\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0180/200: lr: 0.0010000: loss=0.003090 val_acc=0.9710000 val_loss=68.5125961\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0181/200: lr: 0.0010000: loss=0.002812 val_acc=0.9711667 val_loss=68.4692535\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0182/200: lr: 0.0010000: loss=0.002568 val_acc=0.9713333 val_loss=68.4456024\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0183/200: lr: 0.0010000: loss=0.002354 val_acc=0.9713333 val_loss=68.3945541\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0184/200: lr: 0.0010000: loss=0.002165 val_acc=0.9713333 val_loss=68.3848114\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0185/200: lr: 0.0010000: loss=0.001992 val_acc=0.9711667 val_loss=68.3561478\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0186/200: lr: 0.0010000: loss=0.001835 val_acc=0.9711667 val_loss=68.3406677\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0187/200: lr: 0.0010000: loss=0.001695 val_acc=0.9710000 val_loss=68.3142242\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0188/200: lr: 0.0010000: loss=0.001563 val_acc=0.9711667 val_loss=68.2851562\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0189/200: lr: 0.0010000: loss=0.001443 val_acc=0.9711667 val_loss=68.3020630\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0190/200: lr: 0.0010000: loss=0.001337 val_acc=0.9710000 val_loss=68.3046188\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0191/200: lr: 0.0010000: loss=0.001237 val_acc=0.9710000 val_loss=68.2745819\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0192/200: lr: 0.0010000: loss=0.001143 val_acc=0.9710000 val_loss=68.3109131\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0193/200: lr: 0.0010000: loss=0.001057 val_acc=0.9708333 val_loss=68.3011246\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0194/200: lr: 0.0010000: loss=0.000978 val_acc=0.9711667 val_loss=68.2993088\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0195/200: lr: 0.0010000: loss=0.000903 val_acc=0.9710000 val_loss=68.3257370\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0196/200: lr: 0.0010000: loss=0.000836 val_acc=0.9713333 val_loss=68.3065033\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0197/200: lr: 0.0010000: loss=0.000772 val_acc=0.9711667 val_loss=68.3534546\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0198/200: lr: 0.0010000: loss=0.000714 val_acc=0.9713333 val_loss=68.3722534\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0199/200: lr: 0.0010000: loss=0.000660 val_acc=0.9715000 val_loss=68.3830795\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0200/200: lr: 0.0010000: loss=0.000611 val_acc=0.9716667 val_loss=68.4003601\n",
      "DEBUG:MLP.torch_mlp_clf:Training complete in 1m 39s\n",
      "DEBUG:MLP.torch_mlp_clf:Best val_acc@31 = 0.9726666666666667\n",
      "DEBUG:MLP.torch_mlp_clf:Best val_loss@31 = 42.39604187011719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9748"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_train = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "org_test = torchvision.datasets.MNIST('data', train=False)\n",
    "X, y = org_train.data.view(-1, 28*28).numpy(), org_train.targets.numpy()\n",
    "test_X, test_y = org_test.data.view(-1, 28*28).numpy(), org_test.targets.numpy()\n",
    "\n",
    "clf = TorchMLPClassifier()\n",
    "clf.fit(X, y)\n",
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of manual split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples 60000 -> train:valid = 59,000:1,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0001/200: lr: 0.0010000: loss=68.524992 val_acc=0.9720000 val_loss=26.8803234\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0002/200: lr: 0.0010000: loss=26.437814 val_acc=0.9720000 val_loss=23.4838543\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0003/200: lr: 0.0010000: loss=18.369504 val_acc=0.9730000 val_loss=22.0110855\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0004/200: lr: 0.0010000: loss=13.644663 val_acc=0.9790000 val_loss=21.4917431\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0005/200: lr: 0.0010000: loss=10.617735 val_acc=0.9770000 val_loss=22.6902523\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0006/200: lr: 0.0010000: loss=8.305120 val_acc=0.9790000 val_loss=23.5972958\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0007/200: lr: 0.0010000: loss=6.567766 val_acc=0.9790000 val_loss=22.1792336\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0008/200: lr: 0.0010000: loss=5.116318 val_acc=0.9800000 val_loss=23.0932636\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0009/200: lr: 0.0010000: loss=4.093574 val_acc=0.9790000 val_loss=25.8899593\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0010/200: lr: 0.0010000: loss=3.310314 val_acc=0.9770000 val_loss=24.7098846\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0011/200: lr: 0.0010000: loss=2.754835 val_acc=0.9810000 val_loss=26.0511513\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0012/200: lr: 0.0010000: loss=2.181635 val_acc=0.9810000 val_loss=26.4866009\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0013/200: lr: 0.0010000: loss=1.727687 val_acc=0.9780000 val_loss=27.5053654\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0014/200: lr: 0.0010000: loss=1.540595 val_acc=0.9730000 val_loss=29.7777195\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0015/200: lr: 0.0010000: loss=1.407308 val_acc=0.9760000 val_loss=30.7486935\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0016/200: lr: 0.0010000: loss=1.150459 val_acc=0.9790000 val_loss=33.3818855\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0017/200: lr: 0.0010000: loss=0.790009 val_acc=0.9770000 val_loss=32.3814049\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0018/200: lr: 0.0010000: loss=0.599962 val_acc=0.9760000 val_loss=31.5592690\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0019/200: lr: 0.0010000: loss=0.424797 val_acc=0.9790000 val_loss=31.8747311\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0020/200: lr: 0.0010000: loss=0.395893 val_acc=0.9770000 val_loss=33.7069969\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0021/200: lr: 0.0010000: loss=0.316485 val_acc=0.9790000 val_loss=33.7690201\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0022/200: lr: 0.0010000: loss=0.231813 val_acc=0.9780000 val_loss=34.5590172\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0023/200: lr: 0.0010000: loss=0.217656 val_acc=0.9770000 val_loss=35.7641449\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0024/200: lr: 0.0010000: loss=0.206069 val_acc=0.9790000 val_loss=35.6883736\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0025/200: lr: 0.0010000: loss=0.195292 val_acc=0.9750000 val_loss=38.9116211\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0026/200: lr: 0.0010000: loss=6.354748 val_acc=0.9790000 val_loss=62.5463371\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0027/200: lr: 0.0010000: loss=5.601813 val_acc=0.9770000 val_loss=57.7834129\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0028/200: lr: 0.0010000: loss=2.349792 val_acc=0.9780000 val_loss=60.6727257\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0029/200: lr: 0.0010000: loss=0.971213 val_acc=0.9800000 val_loss=59.8354683\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0030/200: lr: 0.0010000: loss=0.247245 val_acc=0.9790000 val_loss=59.7946663\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0031/200: lr: 0.0010000: loss=0.136364 val_acc=0.9800000 val_loss=59.8210220\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0032/200: lr: 0.0010000: loss=0.178571 val_acc=0.9790000 val_loss=60.1906738\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0033/200: lr: 0.0010000: loss=0.089593 val_acc=0.9790000 val_loss=60.5957413\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0034/200: lr: 0.0010000: loss=0.071471 val_acc=0.9790000 val_loss=60.4930916\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0035/200: lr: 0.0010000: loss=0.063954 val_acc=0.9790000 val_loss=60.1584358\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0036/200: lr: 0.0010000: loss=0.058684 val_acc=0.9790000 val_loss=60.8474503\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0037/200: lr: 0.0010000: loss=0.053092 val_acc=0.9790000 val_loss=60.6794548\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0038/200: lr: 0.0010000: loss=0.049554 val_acc=0.9780000 val_loss=61.0589180\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0039/200: lr: 0.0010000: loss=0.045165 val_acc=0.9780000 val_loss=61.7246819\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0040/200: lr: 0.0010000: loss=0.041692 val_acc=0.9780000 val_loss=61.7837219\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0041/200: lr: 0.0010000: loss=0.037582 val_acc=0.9790000 val_loss=61.9591675\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0042/200: lr: 0.0010000: loss=0.034311 val_acc=0.9780000 val_loss=62.6522293\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0043/200: lr: 0.0010000: loss=0.204888 val_acc=0.9770000 val_loss=66.9924316\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0044/200: lr: 0.0010000: loss=6.131987 val_acc=0.9810000 val_loss=68.0626831\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0045/200: lr: 0.0010000: loss=2.124746 val_acc=0.9780000 val_loss=62.8277855\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0046/200: lr: 0.0010000: loss=0.628002 val_acc=0.9800000 val_loss=61.9054337\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0047/200: lr: 0.0010000: loss=0.161995 val_acc=0.9810000 val_loss=60.4123192\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0048/200: lr: 0.0010000: loss=0.070795 val_acc=0.9810000 val_loss=60.4069786\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0049/200: lr: 0.0010000: loss=0.043309 val_acc=0.9800000 val_loss=60.6998863\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0050/200: lr: 0.0010000: loss=0.037222 val_acc=0.9800000 val_loss=60.7341805\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0051/200: lr: 0.0010000: loss=0.032996 val_acc=0.9800000 val_loss=60.7012711\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0052/200: lr: 0.0010000: loss=0.029890 val_acc=0.9800000 val_loss=60.7800179\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0053/200: lr: 0.0010000: loss=0.027238 val_acc=0.9790000 val_loss=60.8139534\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0054/200: lr: 0.0010000: loss=0.024926 val_acc=0.9800000 val_loss=60.6783409\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0055/200: lr: 0.0010000: loss=0.022857 val_acc=0.9800000 val_loss=60.8211670\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0056/200: lr: 0.0010000: loss=0.021004 val_acc=0.9800000 val_loss=60.8765755\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0057/200: lr: 0.0010000: loss=0.019357 val_acc=0.9790000 val_loss=60.9914742\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0058/200: lr: 0.0010000: loss=0.017857 val_acc=0.9800000 val_loss=61.2434044\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0059/200: lr: 0.0010000: loss=0.016298 val_acc=0.9790000 val_loss=60.9545708\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0060/200: lr: 0.0010000: loss=0.015097 val_acc=0.9800000 val_loss=61.1712418\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0061/200: lr: 0.0010000: loss=0.013840 val_acc=0.9800000 val_loss=61.1616783\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0062/200: lr: 0.0010000: loss=0.012662 val_acc=0.9800000 val_loss=61.0790367\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0063/200: lr: 0.0010000: loss=0.011683 val_acc=0.9800000 val_loss=61.6201248\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0064/200: lr: 0.0010000: loss=0.010706 val_acc=0.9800000 val_loss=61.5968018\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0065/200: lr: 0.0010000: loss=0.009791 val_acc=0.9800000 val_loss=61.5400276\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0066/200: lr: 0.0010000: loss=0.008970 val_acc=0.9800000 val_loss=61.8291893\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0067/200: lr: 0.0010000: loss=0.008357 val_acc=0.9810000 val_loss=62.0277214\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0068/200: lr: 0.0010000: loss=0.007238 val_acc=0.9800000 val_loss=62.2986565\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0069/200: lr: 0.0010000: loss=0.006658 val_acc=0.9800000 val_loss=62.8959961\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0070/200: lr: 0.0010000: loss=0.006071 val_acc=0.9800000 val_loss=62.9754639\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0071/200: lr: 0.0010000: loss=0.730207 val_acc=0.9800000 val_loss=76.7881088\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0072/200: lr: 0.0010000: loss=6.967892 val_acc=0.9780000 val_loss=72.6606979\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0073/200: lr: 0.0010000: loss=2.956435 val_acc=0.9780000 val_loss=76.6449585\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0074/200: lr: 0.0010000: loss=1.200569 val_acc=0.9790000 val_loss=76.5084610\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0075/200: lr: 0.0010000: loss=1.617311 val_acc=0.9770000 val_loss=74.8766251\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0076/200: lr: 0.0010000: loss=0.668029 val_acc=0.9780000 val_loss=71.3467102\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0077/200: lr: 0.0010000: loss=0.027184 val_acc=0.9780000 val_loss=71.6620865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0078/200: lr: 0.0010000: loss=0.020651 val_acc=0.9780000 val_loss=71.7445602\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0079/200: lr: 0.0010000: loss=0.017936 val_acc=0.9780000 val_loss=71.7085495\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0080/200: lr: 0.0010000: loss=0.015924 val_acc=0.9780000 val_loss=71.7433090\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0081/200: lr: 0.0010000: loss=0.014371 val_acc=0.9780000 val_loss=71.7714386\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0082/200: lr: 0.0010000: loss=0.013069 val_acc=0.9780000 val_loss=71.7210236\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0083/200: lr: 0.0010000: loss=0.011967 val_acc=0.9780000 val_loss=71.4965591\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0084/200: lr: 0.0010000: loss=0.010990 val_acc=0.9790000 val_loss=71.7119751\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0085/200: lr: 0.0010000: loss=0.010095 val_acc=0.9790000 val_loss=71.6247864\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0086/200: lr: 0.0010000: loss=0.009322 val_acc=0.9790000 val_loss=71.6611786\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0087/200: lr: 0.0010000: loss=0.008613 val_acc=0.9790000 val_loss=71.6422348\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0088/200: lr: 0.0010000: loss=0.007854 val_acc=0.9790000 val_loss=71.6540680\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0089/200: lr: 0.0010000: loss=0.007279 val_acc=0.9800000 val_loss=71.6045151\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0090/200: lr: 0.0010000: loss=0.006738 val_acc=0.9800000 val_loss=71.5916367\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0091/200: lr: 0.0010000: loss=0.006175 val_acc=0.9800000 val_loss=71.6265182\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0092/200: lr: 0.0010000: loss=0.005691 val_acc=0.9800000 val_loss=71.4372787\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0093/200: lr: 0.0010000: loss=0.005240 val_acc=0.9800000 val_loss=71.4095383\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0094/200: lr: 0.0010000: loss=0.004836 val_acc=0.9800000 val_loss=71.6188889\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0095/200: lr: 0.0010000: loss=0.004431 val_acc=0.9800000 val_loss=71.5822067\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0096/200: lr: 0.0010000: loss=0.004074 val_acc=0.9800000 val_loss=71.6511002\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0097/200: lr: 0.0010000: loss=0.003747 val_acc=0.9800000 val_loss=71.7040253\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0098/200: lr: 0.0010000: loss=0.003461 val_acc=0.9800000 val_loss=71.6840439\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0099/200: lr: 0.0010000: loss=0.003159 val_acc=0.9800000 val_loss=71.6870346\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0100/200: lr: 0.0010000: loss=0.002872 val_acc=0.9790000 val_loss=71.9691772\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0101/200: lr: 0.0010000: loss=0.002630 val_acc=0.9790000 val_loss=71.9833603\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0102/200: lr: 0.0010000: loss=0.002397 val_acc=0.9800000 val_loss=71.7832870\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0103/200: lr: 0.0010000: loss=0.002191 val_acc=0.9790000 val_loss=71.7089157\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0104/200: lr: 0.0010000: loss=0.001992 val_acc=0.9800000 val_loss=71.3139572\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0105/200: lr: 0.0010000: loss=0.002481 val_acc=0.9800000 val_loss=71.2166824\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0106/200: lr: 0.0010000: loss=6.081398 val_acc=0.9740000 val_loss=76.5894699\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0107/200: lr: 0.0010000: loss=4.129440 val_acc=0.9800000 val_loss=78.8923569\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0108/200: lr: 0.0010000: loss=1.184608 val_acc=0.9800000 val_loss=87.7479858\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0109/200: lr: 0.0010000: loss=0.957187 val_acc=0.9780000 val_loss=83.7106552\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0110/200: lr: 0.0010000: loss=0.156576 val_acc=0.9790000 val_loss=81.9714890\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0111/200: lr: 0.0010000: loss=0.037123 val_acc=0.9800000 val_loss=81.7489395\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0112/200: lr: 0.0010000: loss=0.011454 val_acc=0.9800000 val_loss=81.7193832\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0113/200: lr: 0.0010000: loss=0.009481 val_acc=0.9800000 val_loss=81.5468674\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0114/200: lr: 0.0010000: loss=0.008341 val_acc=0.9810000 val_loss=81.4507675\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0115/200: lr: 0.0010000: loss=0.007483 val_acc=0.9800000 val_loss=81.3504333\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0116/200: lr: 0.0010000: loss=0.006794 val_acc=0.9790000 val_loss=81.2085648\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0117/200: lr: 0.0010000: loss=0.006206 val_acc=0.9790000 val_loss=81.1716080\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0118/200: lr: 0.0010000: loss=0.005695 val_acc=0.9780000 val_loss=81.0219498\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0119/200: lr: 0.0010000: loss=0.005244 val_acc=0.9780000 val_loss=80.9538651\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0120/200: lr: 0.0010000: loss=0.004833 val_acc=0.9780000 val_loss=80.7976837\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0121/200: lr: 0.0010000: loss=0.004462 val_acc=0.9780000 val_loss=80.5936661\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0122/200: lr: 0.0010000: loss=0.004101 val_acc=0.9780000 val_loss=80.4515610\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0123/200: lr: 0.0010000: loss=0.003811 val_acc=0.9780000 val_loss=80.3772812\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0124/200: lr: 0.0010000: loss=0.003508 val_acc=0.9780000 val_loss=80.1314011\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0125/200: lr: 0.0010000: loss=0.003246 val_acc=0.9780000 val_loss=80.0015030\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0126/200: lr: 0.0010000: loss=0.002991 val_acc=0.9780000 val_loss=79.8317261\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0127/200: lr: 0.0010000: loss=0.002771 val_acc=0.9780000 val_loss=79.5375748\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0128/200: lr: 0.0010000: loss=0.002565 val_acc=0.9780000 val_loss=79.2796097\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0129/200: lr: 0.0010000: loss=0.002344 val_acc=0.9780000 val_loss=79.1782761\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0130/200: lr: 0.0010000: loss=0.002195 val_acc=0.9780000 val_loss=79.0853500\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0131/200: lr: 0.0010000: loss=0.001995 val_acc=0.9790000 val_loss=78.8521271\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0132/200: lr: 0.0010000: loss=0.001848 val_acc=0.9790000 val_loss=78.6245804\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0133/200: lr: 0.0010000: loss=0.001724 val_acc=0.9790000 val_loss=78.2322006\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0134/200: lr: 0.0010000: loss=0.001558 val_acc=0.9790000 val_loss=77.9215927\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0135/200: lr: 0.0010000: loss=0.001427 val_acc=0.9790000 val_loss=77.9195938\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0136/200: lr: 0.0010000: loss=0.001312 val_acc=0.9790000 val_loss=77.5428391\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0137/200: lr: 0.0010000: loss=0.001212 val_acc=0.9790000 val_loss=76.9955444\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0138/200: lr: 0.0010000: loss=0.001097 val_acc=0.9790000 val_loss=76.7319565\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0139/200: lr: 0.0010000: loss=0.001005 val_acc=0.9790000 val_loss=76.5934982\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0140/200: lr: 0.0010000: loss=0.000920 val_acc=0.9790000 val_loss=76.0591583\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0141/200: lr: 0.0010000: loss=0.000835 val_acc=0.9790000 val_loss=75.6351852\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0142/200: lr: 0.0010000: loss=0.000783 val_acc=0.9790000 val_loss=75.2113266\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0143/200: lr: 0.0010000: loss=0.000699 val_acc=0.9790000 val_loss=74.3897858\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0144/200: lr: 0.0010000: loss=0.000636 val_acc=0.9790000 val_loss=74.1455841\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0145/200: lr: 0.0010000: loss=0.000578 val_acc=0.9790000 val_loss=73.7938995\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0146/200: lr: 0.0010000: loss=0.000528 val_acc=0.9790000 val_loss=72.4878159\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0147/200: lr: 0.0010000: loss=0.000488 val_acc=0.9800000 val_loss=72.6829147\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0148/200: lr: 0.0010000: loss=0.000439 val_acc=0.9800000 val_loss=72.9942856\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0149/200: lr: 0.0010000: loss=0.000408 val_acc=0.9800000 val_loss=72.8396454\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0150/200: lr: 0.0010000: loss=0.000390 val_acc=0.9790000 val_loss=73.0230865\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0151/200: lr: 0.0010000: loss=0.000352 val_acc=0.9800000 val_loss=74.1753922\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0152/200: lr: 0.0010000: loss=0.000312 val_acc=0.9810000 val_loss=73.5344849\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0153/200: lr: 0.0010000: loss=6.349328 val_acc=0.9790000 val_loss=116.6839371\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0154/200: lr: 0.0010000: loss=5.921705 val_acc=0.9770000 val_loss=124.3697281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:MLP.torch_mlp_clf:epoch 0155/200: lr: 0.0010000: loss=2.086821 val_acc=0.9790000 val_loss=105.9618454\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0156/200: lr: 0.0010000: loss=0.850581 val_acc=0.9810000 val_loss=111.3593979\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0157/200: lr: 0.0010000: loss=0.364483 val_acc=0.9780000 val_loss=119.1338043\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0158/200: lr: 0.0010000: loss=0.028310 val_acc=0.9790000 val_loss=115.6277390\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0159/200: lr: 0.0010000: loss=0.008198 val_acc=0.9790000 val_loss=115.1190720\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0160/200: lr: 0.0010000: loss=0.006468 val_acc=0.9790000 val_loss=114.9571762\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0161/200: lr: 0.0010000: loss=0.005589 val_acc=0.9790000 val_loss=114.7334518\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0162/200: lr: 0.0010000: loss=0.004953 val_acc=0.9790000 val_loss=114.5865707\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0163/200: lr: 0.0010000: loss=0.004427 val_acc=0.9790000 val_loss=114.4115524\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0164/200: lr: 0.0010000: loss=0.003986 val_acc=0.9800000 val_loss=114.2537613\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0165/200: lr: 0.0010000: loss=0.003604 val_acc=0.9800000 val_loss=114.1250610\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0166/200: lr: 0.0010000: loss=0.003279 val_acc=0.9800000 val_loss=113.9858398\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0167/200: lr: 0.0010000: loss=0.002987 val_acc=0.9800000 val_loss=113.8806152\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0168/200: lr: 0.0010000: loss=0.002736 val_acc=0.9800000 val_loss=113.6634293\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0169/200: lr: 0.0010000: loss=0.002504 val_acc=0.9800000 val_loss=113.5321579\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0170/200: lr: 0.0010000: loss=0.002295 val_acc=0.9800000 val_loss=113.4320068\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0171/200: lr: 0.0010000: loss=0.002100 val_acc=0.9800000 val_loss=113.3172836\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0172/200: lr: 0.0010000: loss=0.001932 val_acc=0.9800000 val_loss=113.2056885\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0173/200: lr: 0.0010000: loss=0.001767 val_acc=0.9810000 val_loss=113.0500031\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0174/200: lr: 0.0010000: loss=0.001629 val_acc=0.9810000 val_loss=112.9110947\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0175/200: lr: 0.0010000: loss=0.001498 val_acc=0.9810000 val_loss=112.7517624\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0176/200: lr: 0.0010000: loss=0.001372 val_acc=0.9800000 val_loss=112.6414185\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0177/200: lr: 0.0010000: loss=0.001262 val_acc=0.9800000 val_loss=112.5560684\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0178/200: lr: 0.0010000: loss=0.001158 val_acc=0.9800000 val_loss=112.3660507\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0179/200: lr: 0.0010000: loss=0.001060 val_acc=0.9800000 val_loss=112.2828598\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0180/200: lr: 0.0010000: loss=0.000973 val_acc=0.9800000 val_loss=112.2540512\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0181/200: lr: 0.0010000: loss=0.000891 val_acc=0.9800000 val_loss=112.0093384\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0182/200: lr: 0.0010000: loss=0.000823 val_acc=0.9800000 val_loss=111.8725357\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0183/200: lr: 0.0010000: loss=0.000750 val_acc=0.9790000 val_loss=111.7675323\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0184/200: lr: 0.0010000: loss=0.000688 val_acc=0.9790000 val_loss=111.5003204\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0185/200: lr: 0.0010000: loss=0.000629 val_acc=0.9780000 val_loss=111.4440689\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0186/200: lr: 0.0010000: loss=0.000578 val_acc=0.9790000 val_loss=111.0840836\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0187/200: lr: 0.0010000: loss=0.000526 val_acc=0.9780000 val_loss=110.8978195\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0188/200: lr: 0.0010000: loss=0.000480 val_acc=0.9780000 val_loss=110.4665756\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0189/200: lr: 0.0010000: loss=0.000441 val_acc=0.9780000 val_loss=110.2388916\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0190/200: lr: 0.0010000: loss=0.000403 val_acc=0.9780000 val_loss=110.1121140\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0191/200: lr: 0.0010000: loss=0.000375 val_acc=0.9780000 val_loss=109.9048843\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0192/200: lr: 0.0010000: loss=0.000339 val_acc=0.9780000 val_loss=109.6529160\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0193/200: lr: 0.0010000: loss=0.000314 val_acc=0.9780000 val_loss=109.1198654\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0194/200: lr: 0.0010000: loss=0.000286 val_acc=0.9780000 val_loss=108.8657227\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0195/200: lr: 0.0010000: loss=0.000262 val_acc=0.9780000 val_loss=108.6344223\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0196/200: lr: 0.0010000: loss=0.000240 val_acc=0.9780000 val_loss=108.2317886\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0197/200: lr: 0.0010000: loss=0.000221 val_acc=0.9780000 val_loss=107.6220703\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0198/200: lr: 0.0010000: loss=0.000208 val_acc=0.9780000 val_loss=107.2590332\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0199/200: lr: 0.0010000: loss=0.000190 val_acc=0.9780000 val_loss=107.0161896\n",
      "DEBUG:MLP.torch_mlp_clf:epoch 0200/200: lr: 0.0010000: loss=0.000173 val_acc=0.9780000 val_loss=106.7995834\n",
      "DEBUG:MLP.torch_mlp_clf:Training complete in 1m 39s\n",
      "DEBUG:MLP.torch_mlp_clf:Best val_acc@175 = 0.981\n",
      "DEBUG:MLP.torch_mlp_clf:Best val_loss@175 = 112.75176239013672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9753"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_train = torchvision.datasets.MNIST('data', train=True, download=True)\n",
    "org_test = torchvision.datasets.MNIST('data', train=False)\n",
    "X, y = org_train.data.view(-1, 28*28).numpy(), org_train.targets.numpy()\n",
    "test_X, test_y = org_test.data.view(-1, 28*28).numpy(), org_test.targets.numpy()\n",
    "\n",
    "print(f'# of training examples {len(org_train)} -> train:valid = 59000:1000')\n",
    "val_idxs = list(range(59000, 60000))\n",
    "\n",
    "clf = TorchMLPClassifier()\n",
    "clf.fit(X, y, val_idxs=val_idxs)\n",
    "clf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
