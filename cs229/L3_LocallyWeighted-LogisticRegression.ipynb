{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/het9HFqo1TQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Weighted Regression\n",
    "\n",
    "Parametric - Non-parametric learning algorithm.\n",
    "\n",
    "- Parametric: Fit fixed set of parameters( theta) to data. No matter how big data is, parameter size is fixed.\n",
    "- Non-parametric: Keep data = need to keep data even when data size is massive.\n",
    "\n",
    "$$\\sum_{i=1}^{M}{w^{(i)}(y^{(i)} - \\theta^Tx^{(i)})^2}$$\n",
    "\n",
    "where $w^{(i)}$ is a weighting function.\n",
    "\n",
    "$$w^{(i)} = e^{-\\frac{(x^{(i)} - x)^2}{2 \\tau^2}}$$\n",
    "\n",
    "If $|x^{(i)} - x|$ is small, $w^{(i)}\\approx 1$.\n",
    "If $|x^{(i)} - x|$ is large, $w^{(i)}\\approx 0$.\n",
    "\n",
    "$w^{(i)}$ looks a lot like Gaussian density, but nothing to do with it. Gaussian probability density function have to integrate to 1, this is not.\n",
    "\n",
    "### $\\tau$\n",
    "\n",
    "Hyper parameter $\\tau$ decides weighting to neighbor samples - if $\\tau$ is too broad, you end up fover-smoothing data, if $\\tau$ is too thin, you end up fitting a very jagged fit to the data.\n",
    "\n",
    "### When to use this\n",
    "\n",
    "- Number of features is not too big, 2 or 3 or something.\n",
    "- And you have a lot of data.\n",
    "- Don't want to think about which features to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why least squared error? (for linear regression)\n",
    "\n",
    "Assume housing price:\n",
    "\n",
    "$$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$$\n",
    "\n",
    "$\\epsilon$ is unmodeled effects, random noise. Assume this is distributed Gausian (normal distribution) would mean 0 and co-variance sigma squared.\n",
    "\n",
    "$$\\epsilon^{(i)} \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "$$P(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp{\\frac{\\epsilon^{(i)^2}}{2\\sigma^2}}$$\n",
    "\n",
    "i.i.d. assumption, implies:\n",
    "\n",
    "$$P(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp{\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}}$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$y^{(i)}|x^{(i)};\\theta \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "- $y^{(i)}|x^{(i)};\\theta$ - this semicolon means \"parameterized by\".\n",
    "- $y^{(i)}|x^{(i)},\\theta$ - this means \"conditioning on theta\". --> can use with $\\epsilon$ but with $\\theta$\n",
    "\n",
    "Likelihood $L(\\theta) = P(\\bar{y}|x;\\theta)$ is product: (from i.i.d. assumption)\n",
    "\n",
    "$$L(\\theta) = P(\\bar{y}|x;\\theta) = \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta)\n",
    "= \\prod_{m=1}^{m} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp{\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower case $l(\\theta)$ is log likelihood $\\log L(\\theta)$.\n",
    "\n",
    "$$l(\\theta) = \\log \\prod_{m=1}^{m} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp{\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}}\n",
    " = \\sum_{m=1}^{m} \\frac{1}{\\sqrt{2 \\pi}\\sigma} \\exp{\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}}$$\n",
    " \n",
    "-  MLE: Maximum likelihood estimation; Choose $\\theta$ to maximize $L(\\theta)$.\n",
    "    - Given a dataste, how would you estimate $\\theta$?\n",
    "    - One natural way to do that is choosing $\\theta$ whatever value of theta has a highest likelihood $L(\\theta)$.\n",
    "    - In other words, choosing a value of $\\theta$ so that the value of theta maximizes the probability of the data.\n",
    "    - Maximizing log likelihood is easier, and log is monotonically increasing function, then it should maximize the likelihood.\n",
    "\n",
    "Concluded as:\n",
    "\n",
    "$$l(\\theta) = m \\log \\frac{1}{\\sqrt{2 \\pi} \\sigma} + \\sum_{i=1}^{m}{-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}}$$\n",
    "\n",
    "First term is constant. So maximizing second term is, minimizing:\n",
    "\n",
    "$$\\frac{1}{2}\\sum_{i=1}^{m}{(y^{(i)} - \\theta^T x^{(i)})^2} = J(\\theta)$$\n",
    "\n",
    "Cost function $J(\\theta)$ for linear regression.\n",
    "\n",
    "Choose a value of theta to minimize least squares error, that's just finding the maximum likelihood estimate for the parameter theta under this set of assumption we made, that the error terms are Gaussian and i.i.d..\n",
    "\n",
    "### Likelihood and probability difference\n",
    "\n",
    "- If we fix data, it's likelihood of parameter estimation.\n",
    "- If we fix parameter, it's probability of prediction of y from input x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (Logistic Regression)\n",
    "\n",
    "$y \\in \\{0, 1\\}$\n",
    "\n",
    "$$h_\\theta(x) \\in [0, 1]$$\n",
    "\n",
    "$$h_\\theta(x) = g(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$\n",
    "\n",
    "$1/(1 + e^z)$ is \"sigmoid\" or \"logistic\" function, it is design choice of machine learning model.\n",
    "\n",
    "Assumption:\n",
    "\n",
    "- $p(y=1|x; \\theta) = h_\\theta (x)$\n",
    "- $p(y=0|x; \\theta) = 1 - h_\\theta (x)$\n",
    "\n",
    "compressed in one equation:\n",
    "\n",
    "$$p(y|x; \\theta) = h(x)^y (1 - h(x))^{1-y}$$\n",
    "\n",
    "$$ L(\\theta) = p(Y|x;\\theta) = \\prod_{i=1}^{m}{h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}}}$$\n",
    "\n",
    "$$ l(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{m}{y^{(i)} \\log h_\\theta(x^{(i)}) +  (1-y^{(i)}) \\log (1 - h_\\theta(x^{(i)}))}$$\n",
    "\n",
    "Choose the value of theta to maximize $l(\\theta)$.\n",
    "\n",
    "Batch gradient ascent:\n",
    "\n",
    "We will update the parameters $\\theta_j$ according to $\\theta_j$ plus partial derivative with respect to the log-likelihood.\n",
    "\n",
    "$$ \\theta_j = \\theta_j + \\alpha \\frac{\\partial}{\\partial\\theta_j}{l(\\theta)}$$\n",
    "\n",
    "$$ \\theta_j = \\theta_j + \\alpha \\sum_{i=1}^{m}{(y_j^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}} $$\n",
    "\n",
    "Difference from linear regression:\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}{J(\\theta)}$$\n",
    "\n",
    "For cost function is $J(\\theta) = 1/2 \\sum_{i=0}^{M}(y  - h(x))$. Two differences:\n",
    "\n",
    "- Gradient ascend - descent.\n",
    "- Maximizing log-likelihood - minimizing cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Much bigger jump than gradient descent.\n",
    "\n",
    "$$ \\theta^{(t+1)} =\\theta^{(t)}  - \\frac{f(\\theta^{(t)})}{f'(\\theta^{(t)})}$$\n",
    "\n",
    "let $f(\\theta) = l'(\\theta)$,\n",
    "\n",
    "$$ \\theta^{(t+1)} =\\theta^{(t)}  - \\frac{l'(\\theta^{(t)})}{l''(\\theta^{(t)})}$$\n",
    "\n",
    "\"Quadratic convergence\": 0.01 error --> 0.0001 --> 0.0000001.\n",
    "\n",
    "When \\theta is a vector: (\\theta \\in \\R^{n+1})\n",
    "\n",
    "$$ \\theta^{(t+1)} =\\theta^{(t)}  - H^{-1} \\nabla_{\\theta} l $$\n",
    "\n",
    "Where $H$ is the Hessian matrix \\R^{n+1 x n+1}, \\nabla_{\\theta} l is \\R^{n+1}\n",
    "\n",
    "$$H_{ij} = \\frac{\\partial^2 l}{\\partial\\theta_i \\partial\\theta_j}$$\n",
    "\n",
    "### Rule of thumb\n",
    "\n",
    "- If parameters are up to 10 or 50, Newton's method is useful to make it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
